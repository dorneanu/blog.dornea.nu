<!doctype html>
<html lang="en">
<head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <title>Web Application Survey Tutorial - blog.dornea.nu</title>
  <meta content='Web Application Survey Tutorial - blog.dornea.nu' property='title' />
  <meta content='Web Application Survey Tutorial - blog.dornea.nu' property='og:title' />


<meta property="og:description" content="Suppose you get a list of some URLs and you are asked to &ldquo;investigate&rdquo; them. The list is full of some random URLs related to your company and nobody knows about. You don&rsquo;t have a clue who is responsible for them nor which applications (if any) are running behind them. Sounds like a cool task, ugh?
Well in today&rsquo;s post I&rsquo;ll show you how I&rsquo;ve managed it to minimize the process of analyzing each URL manually and saved me a lot of time automatizing things." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://blog.dornea.nu/2014/08/06/web-application-survey-tutorial/" />


<meta property="article:published_time" content="2014-08-06T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2014-08-06T00:00:00&#43;00:00"/>








<meta name="generator" content="Hugo 0.78.1" />

<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" rel="stylesheet">
<style type="text/css">/*https://coolors.co/afd5aa-f0f2ef-a69f98-3d3d3d-8c6057*/
:root {
  --main-color: #8C6056; 
  --secondary-color: #AFD5AA;
  --logo-text-color: #fff;
  --body-text-color: #3d3d3d;
  --heading-text-color: #383838;
  --background-color: #fff;
}</style>
<link href='http://blog.dornea.nu/css/tachyons.min.css' rel="stylesheet">
<link href='http://blog.dornea.nu/css/styles.css' rel="stylesheet">


<link rel="icon" 
 
  href='http://blog.dornea.nu/favicon.ico'

type="image/x-icon"/>

<link href='http://blog.dornea.nu/feed.xml' rel="alternate" type="application/atom+xml" title="blog.dornea.nu" />
</head>
<body class="global-font">
  <nav class=" flex-ns justify-between border-box pa3 pl3-l pr2-l mt1 mt0-ns" id="navbar">
  <div class="flex">
    <a class="f4 fw6 ttu no-underline dim bg-main-color pv1 ph2 br2" id="site-title" href='http://blog.dornea.nu/' title="Home">blog.dornea.nu</a>
  </div>
  
  <div class=" flex-ns mt2 mt0-ns pv1">
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='http://blog.dornea.nu/' title="Home">Home</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='http://dornea.nu' title="About">About</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='http://blog.dornea.nu/tags' title="Tags">Tags</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='https://brainfck.org' title="Zettelkasten">Zettelkasten</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='https://brainfck.org/bookmarks.html' title="Bookmarks">Bookmarks</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='https://brainfck.org/bib.html' title="Books">Books</a>
    
      <a class="link dim dark-gray f6 dib mr2 mr3-l ttu tracked" href='http://blog.dornea.nu/notes' title="Notes">Notes</a>
    
  </div>
  
</nav>
  
<main class="center mv4 content-width ph3">
  <div class="f3 fw6 heading-color heading-font post-title">Web Application Survey Tutorial</div>
  <p class="silver f6 mt1 mb4 post-meta">
    <time>06 Aug 2014</time> 
     | 
    
    
    tags: [ <a href='http://blog.dornea.nu/tags/ipython' class="link silver">ipython</a> <a href='http://blog.dornea.nu/tags/appsec' class="link silver">appsec</a> <a href='http://blog.dornea.nu/tags/web' class="link silver">web</a> <a href='http://blog.dornea.nu/tags/python' class="link silver">python</a>  ]
    
  </p>
  <div class="lh-copy post-content"><p>Suppose you get a list of some URLs and you are asked to &ldquo;investigate&rdquo; them. The list is full of some random URLs related to your company and nobody knows about. You don&rsquo;t have a clue who is responsible for them nor which applications (if any) are running behind them. Sounds like a cool task, ugh?</p>
<p>Well in today&rsquo;s post I&rsquo;ll show you how I&rsquo;ve managed it to minimize the process of analyzing each URL manually and saved me a lot of time automatizing things.</p>
<h2 id="setup-environment">Setup environment</h2>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">%pylab inline
# &lt;!-- collapse=True --&gt;
import binascii
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import datetime as dt
import time
import ipy_table
import dnslib
import pythonwhois
import urlparse
import tldextract
import json
import os
import sys
import urllib2


from yurl import URL
from urlparse import urlparse
from IPython.display import display_pretty, display_html, display_jpeg, display_png, display_json, display_latex, display_svg

# Ipython settings
pd.set_option(&#39;display.height&#39;, 1000)
pd.set_option(&#39;display.max_rows&#39;, 500)
pd.set_option(&#39;display.max_columns&#39;, 500)
pd.set_option(&#39;display.max_colwidth&#39;, 100)
pd.set_option(&#39;display.width&#39;, 3000)
pd.set_option(&#39;display.column_space&#39;, 1000)

# Change working directory
os.chdir(&#34;/root/work/appsurvey&#34;)
</code></pre></div><pre><code>Populating the interactive namespace from numpy and matplotlib
height has been deprecated.
</code></pre>
<p>First I&rsquo;ll read the list of targets from some CSV file.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># Fetch list of random URLs (found using Google)
response = urllib2.urlopen(&#39;http://files.ianonavy.com/urls.txt&#39;)
targets_row = response.read()

# Create DataFrame
targets = pd.DataFrame([t for t in targets_row.splitlines()], columns=[&#34;Target&#34;])
print(&#34;First 20 entries in the targets list&#34;)
targets[:20]
</code></pre></div><pre><code>First 20 entries in the targets list
</code></pre>
<!-- raw HTML omitted -->
<p>Now I&rsquo;ll split the URLs in several parts:</p>
<ul>
<li>schema (HTTP, FTP, SSH etc.)</li>
<li>host</li>
<li>port</li>
<li>path</li>
<li>query (?q=somevalue etc.)</li>
</ul>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;
# Join root domain + suffix
extract_root_domain =  lambda x: &#39;.&#39;.join(tldextract.extract(x)[1:3])

target_columns = [&#39;scheme&#39;, &#39;userinfo&#39;, &#39;host&#39;, &#39;port&#39;, &#39;path&#39;, &#39;query&#39;, &#39;fragment&#39;, &#39;decoded&#39;]
target_component = [list(URL(t)) for t in targets[&#39;Target&#39;]]

df_targets = pd.DataFrame(target_component, columns=target_columns)
empty_hosts = df_targets[df_targets[&#39;host&#39;] == &#39;&#39;]

# Copy path information to host
for index,row in empty_hosts.iterrows():
    df_targets.ix[index:index][&#39;host&#39;] = df_targets.ix[index:index][&#39;path&#39;]
    df_targets.ix[index:index][&#39;path&#39;] = &#39;&#39;
    
# Extract root tld
df_targets[&#39;root_domain&#39;] = df_targets[&#39;host&#39;].apply(extract_root_domain)

# Drop unnecessary columns
df_targets.drop([&#39;query&#39;, &#39;fragment&#39;, &#39;decoded&#39;], axis=1, inplace=True)

# Write df to file (for later use)
df_targets.to_csv(&#34;targets_df.csv&#34;, sep=&#34;\t&#34;)

print(&#34;First 20 Entries&#34;)
df_targets[:20]
</code></pre></div><pre><code>First 20 Entries
</code></pre>
<!-- raw HTML omitted -->
<h2 id="whois">Whois</h2>
<p>Now get WHOIS information based on data in <code>df_targets</code>:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">%%bash
<span style="color:#6ab825;font-weight:bold">if</span> [ ! -d <span style="color:#ed9d13">&#34;WHOIS&#34;</span> ]; <span style="color:#6ab825;font-weight:bold">then</span>
    mkdir WHOIS
<span style="color:#6ab825;font-weight:bold">fi</span>
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># Get unique values
uniq_roots = df_targets[&#39;root_domain&#39;].unique()
uniq_subdomains = df_targets[&#39;host&#39;].unique()
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;

def date_handler(obj):
    return obj.isoformat() if hasattr(obj, &#39;isoformat&#39;) else obj

target_whois = {}

def fetch_whois(domains):
    &#34;&#34;&#34; Fetch WHOIS information for specified domains (list) &#34;&#34;&#34;
    for d in domains:
        print(&#34;Get WHOIS for\t %s ...&#34; % d)

        # Check if file already exists
        if os.path.isfile(&#34;WHOIS/%s.json&#34; % d):
            print(&#34;File exists already. Aborting.&#34;)
            continue

        try:
            # Get whois information
            whois_data = pythonwhois.get_whois(d)

            # Convert to JSON$
            json_data = json.dumps(whois_data, default=date_handler)

            # Write contents to file
            with open(&#39;WHOIS/%s.json&#39; % d, &#39;w&#39;) as outfile:
              json.dump(json_data, outfile)

            # Sleep for 20s    
            time.sleep(20)
        except:
            print(&#34;[ERROR] Couldn&#39;t retrieve WHOIS for\t %s&#34; % d)
            
# I&#39;ll only fetch the root domains and only the first 20. Feel free to uncomment this
# and adapt it to your needs.
#fetch_whois(uniq_subdomains)
fetch_whois(uniq_roots[:20])
    
</code></pre></div><pre><code>Get WHOIS for	 altpress.org ...
Get WHOIS for	 nzfortress.co.nz ...
Get WHOIS for	 evillasforsale.com ...
Get WHOIS for	 playingenemy.com ...
Get WHOIS for	 richardsonscharts.com ...
Get WHOIS for	 xenith.net ...
Get WHOIS for	 tdbrecords.com ...
Get WHOIS for	 electrichumanproject.com ...
Get WHOIS for	 tweekerchick.blogspot.com ...
Get WHOIS for	 besound.com ...
Get WHOIS for	 porkchopscreenprinting.com ...
Get WHOIS for	 kinseyvisual.com ...
Get WHOIS for	 rathergood.com ...
Get WHOIS for	 lepoint.fr ...
Get WHOIS for	 revhq.com ...
Get WHOIS for	 poprocksandcoke.com ...
Get WHOIS for	 samuraiblue.com ...
Get WHOIS for	 openbsd.org ...
Get WHOIS for	 sysblog.com ...
Get WHOIS for	 voicesofsafety.com ...
</code></pre>
<h2 id="get-all-dns-records">Get all DNS records</h2>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">%%bash
<span style="color:#6ab825;font-weight:bold">if</span> [ ! -d <span style="color:#ed9d13">&#34;DNS&#34;</span> ]; <span style="color:#6ab825;font-weight:bold">then</span>
    mkdir DNS
<span style="color:#6ab825;font-weight:bold">fi</span>
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;
def fetch_dns(domains):
    &#34;&#34;&#34; Fetch all DNS records for specified domains (list) &#34;&#34;&#34;
    for d in domains:
        print(&#34;Dig DNS records for\t %s ...&#34; % d)

        # Check if file already exists
        if os.path.isfile(&#34;DNS/%s.txt&#34; % d):
            print(&#34;File exists already. Aborting.&#34;)
            continue
            
        # Get DNS info
        dig_data = !dig +nocmd $d any +multiline +noall +answer
        dig_output = &#34;\n&#34;.join(dig_data)
        
        # Write contents to file
        with open(&#39;DNS/%s.txt&#39; % d, &#39;w&#39;) as outfile:
            outfile.write(dig_output)
            outfile.close()
        
        time.sleep(5)
        
# I&#39;ll only fetch the root domains and only the first 20. Feel free to uncomment this
# and adapt it to your needs.
#fetch_dns(uniq_subdomains)
fetch_dns(uniq_roots[:20])
</code></pre></div><pre><code>Dig DNS records for	 altpress.org ...
Dig DNS records for	 nzfortress.co.nz ...
Dig DNS records for	 evillasforsale.com ...
Dig DNS records for	 playingenemy.com ...
Dig DNS records for	 richardsonscharts.com ...
Dig DNS records for	 xenith.net ...
Dig DNS records for	 tdbrecords.com ...
Dig DNS records for	 electrichumanproject.com ...
Dig DNS records for	 tweekerchick.blogspot.com ...
Dig DNS records for	 besound.com ...
Dig DNS records for	 porkchopscreenprinting.com ...
Dig DNS records for	 kinseyvisual.com ...
Dig DNS records for	 rathergood.com ...
Dig DNS records for	 lepoint.fr ...
Dig DNS records for	 revhq.com ...
Dig DNS records for	 poprocksandcoke.com ...
Dig DNS records for	 samuraiblue.com ...
Dig DNS records for	 openbsd.org ...
Dig DNS records for	 sysblog.com ...
Dig DNS records for	 voicesofsafety.com ...
</code></pre>
<h2 id="read-whois-information">Read WHOIS information</h2>
<p>After collecting the data I&rsquo;ll try to manipulate data in a pythonic way in order to export it later to some useful format like Excel. I&rsquo;ll therefor read the collected data from every single file, merge the data and create a <code>DataFrame</code>.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;
from pprint import pprint

# Global DF frames
frames = []

def read_whois(domains):
    for d in domains:
        print(&#34;Reading WHOIS for\t %s&#34; % d)
        
        try:
            with open(&#39;WHOIS/%s.json&#39; % d, &#39;r&#39;) as inputfile:
                whois = json.loads(json.load(inputfile))

                # Delete raw record
                whois.pop(&#39;raw&#39;, None)

                data = []
                
                # Iterate contacts -&gt; tech
                if whois[&#39;contacts&#39;][&#39;tech&#39;]:
                    for i in whois[&#39;contacts&#39;][&#39;tech&#39;]:
                        data.append([d, &#39;contacts&#39;, &#39;tech&#39;, i, whois[&#39;contacts&#39;][&#39;tech&#39;][i]])

                # Iterate contacts -&gt; admin
                if whois[&#39;contacts&#39;][&#39;admin&#39;]:
                    for i in whois[&#39;contacts&#39;][&#39;admin&#39;]:
                        data.append([d, &#39;contacts&#39;, &#39;admin&#39;, i, whois[&#39;contacts&#39;][&#39;admin&#39;][i]])

                # Nameservers
                if &#34;nameservers&#34; in whois:
                    for i in whois[&#39;nameservers&#39;]:
                        data.append([d, &#39;nameservers&#39;, &#39;&#39;, &#39;&#39;, i])

                # Create DF only if data is not empty
                if data:
                    df = pd.DataFrame(data, columns=[&#39;domain&#39;, &#39;element&#39;, &#39;type&#39;, &#39;field&#39;, &#39;value&#39;])
                    frames.append(df)

                # Close file
                inputfile.close()
        except:
            print(&#34;[ERROR] Couldn&#39;t read WHOIS for\t %s&#34; % d)

#read_whois(uniq_subdomains)
read_whois(uniq_roots[:20])
</code></pre></div><pre><code>Reading WHOIS for	 altpress.org
Reading WHOIS for	 nzfortress.co.nz
Reading WHOIS for	 evillasforsale.com
Reading WHOIS for	 playingenemy.com
Reading WHOIS for	 richardsonscharts.com
Reading WHOIS for	 xenith.net
Reading WHOIS for	 tdbrecords.com
Reading WHOIS for	 electrichumanproject.com
Reading WHOIS for	 tweekerchick.blogspot.com
Reading WHOIS for	 besound.com
Reading WHOIS for	 porkchopscreenprinting.com
Reading WHOIS for	 kinseyvisual.com
Reading WHOIS for	 rathergood.com
Reading WHOIS for	 lepoint.fr
Reading WHOIS for	 revhq.com
Reading WHOIS for	 poprocksandcoke.com
Reading WHOIS for	 samuraiblue.com
Reading WHOIS for	 openbsd.org
Reading WHOIS for	 sysblog.com
Reading WHOIS for	 voicesofsafety.com
</code></pre>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_whois = pd.concat(frames)
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_whois.set_index([&#39;domain&#39;, &#39;element&#39;, &#39;type&#39;, &#39;field&#39;])
</code></pre></div><!-- raw HTML omitted -->
<h2 id="read-dns-information">Read DNS information</h2>
<p>Do the same with the DNS files&hellip;</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;
from pprint import pprint
import re
import traceback

# Global DF frames
frames = []

def read_dns(domains):
    for d in domains:
        print(&#34;Reading WHOIS for\t %s&#34; % d)
        data = []
        try:
            with open(&#39;DNS/%s.txt&#39; % d, &#39;r&#39;) as inputfile:
                dns = inputfile.read()
                
                for l in dns.splitlines():
                    records = l.split()
                    
                    # Check only for NS, MX, A, CNAME, TXT
                    a = re.compile(&#34;^(NS|MX|A|CNAME|TXT)$&#34;)
                    if len(records) &gt;= 4:
                        if a.match(records[3]):
                            data.append([d, records[3], records[4]])
                
                # Create DF only if data is not empty
                if data:
                    df = pd.DataFrame(data, columns=[&#39;domain&#39;, &#39;dns_record&#39;, &#39;value&#39;])
                    frames.append(df)      
                    
                # Close file
                inputfile.close()
                
        except Exception, err:
            print(&#34;[ERROR] Couldn&#39;t read WHOIS for\t %s&#34; % d)
            traceback.print_exc()

#read_dns(uniq_subdomains)            
read_dns(uniq_roots[:20])
</code></pre></div><pre><code>Reading WHOIS for	 altpress.org
Reading WHOIS for	 nzfortress.co.nz
Reading WHOIS for	 evillasforsale.com
Reading WHOIS for	 playingenemy.com
Reading WHOIS for	 richardsonscharts.com
Reading WHOIS for	 xenith.net
Reading WHOIS for	 tdbrecords.com
Reading WHOIS for	 electrichumanproject.com
Reading WHOIS for	 tweekerchick.blogspot.com
Reading WHOIS for	 besound.com
Reading WHOIS for	 porkchopscreenprinting.com
Reading WHOIS for	 kinseyvisual.com
Reading WHOIS for	 rathergood.com
Reading WHOIS for	 lepoint.fr
Reading WHOIS for	 revhq.com
Reading WHOIS for	 poprocksandcoke.com
Reading WHOIS for	 samuraiblue.com
Reading WHOIS for	 openbsd.org
Reading WHOIS for	 sysblog.com
Reading WHOIS for	 voicesofsafety.com
</code></pre>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_dns = pd.concat(frames)
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_dns.set_index([&#39;domain&#39;, &#39;dns_record&#39;])
</code></pre></div><!-- raw HTML omitted -->
<h2 id="connect-to-targets">Connect to targets</h2>
<p>For every single target I&rsquo;ll connect to it per HTTP(s) using <code>urllib2</code> and store the HTTP headers.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"># &lt;!-- collapse=True --&gt;
import urllib2
import httplib


c_targets = [t for t in targets[&#39;Target&#39;][:20]]
frames = []

# Collect here all URLs failed to connect to
error_urls = []

def send_request(target, data):
    &#34;&#34;&#34; Sends a single request to the target &#34;&#34;&#34;            
    
    # Set own headers
    headers = {&#39;User-Agent&#39; : &#39;Mozilla 5.10&#39;}

    # Create request
    request = urllib2.Request(target, None, headers)
    
    # Default response
    response = None
        
    try:
        # Send request
        response = urllib2.urlopen(request, timeout=5)
        
        # Add headers
        for h in response.info():
            data.append([target, response.code, h, response.info()[h]])
        
    except urllib2.HTTPError, e:
        print(&#39;[ERROR] HTTPError = &#39; + str(e.code))
        data.append([target, e.code, &#39;&#39;, &#39;&#39;])
            
    except urllib2.URLError, e:
        print(&#39;[ERROR] URLError = &#39; + str(e.reason))
        data.append([target, e.reason, &#39;&#39;, &#39;&#39;])
            
    except ValueError, e:
        # Most probably the target didn&#39;t have any schema
        # So send the request again with HTTP
        error_urls.append(target)
        print(&#39;[ERROR] ValueError = &#39; + e.message)
            
    except httplib.HTTPException, e:
        print(&#39;[ERROR] HTTPException&#39;)
            
    except Exception:
        import traceback
        print(&#39;[ERROR] Exception: &#39; + traceback.format_exc())
        
    finally:
        return response
        

    
    
def open_connection(targets):
    &#34;&#34;&#34; Iterate through targets and send requests &#34;&#34;&#34;
    data = []
    for t in targets:
        print(&#34;Connecting to\t %s&#34; % t)
        
        response = send_request(t, data)
        
    # Create DF only if data is not empty
    if data:
        df = pd.DataFrame(data, columns=[&#39;url&#39;, &#39;response&#39;, &#39;header&#39;, &#39;value&#39;])
        frames.append(df)    
        

# Open connection to targets and collect information
open_connection(c_targets)

# If there are any urls not having been tested, then
# prepend http:// to &lt;target&gt; and run again
new_targets =  [&#34;http://&#34;+u for u in error_urls]
open_connection(new_targets)
</code></pre></div><pre><code>Connecting to	 http://www.altpress.org/
Connecting to	 http://www.nzfortress.co.nz
Connecting to	 http://www.evillasforsale.com
Connecting to	 http://www.playingenemy.com/
[ERROR] URLError = timed out
Connecting to	 http://www.richardsonscharts.com
Connecting to	 http://www.xenith.net
[ERROR] Exception: Traceback (most recent call last):
  File &quot;&lt;ipython-input-19-d057092f77b5&gt;&quot;, line 26, in send_request
    response = urllib2.urlopen(request, timeout=5)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 401, in open
    response = self._open(req, data)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 419, in _open
    '_open', req)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 379, in _call_chain
    result = func(*args)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1211, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1184, in do_open
    r = h.getresponse(buffering=True)
  File &quot;/usr/lib/python2.7/httplib.py&quot;, line 1034, in getresponse
    response.begin()
  File &quot;/usr/lib/python2.7/httplib.py&quot;, line 407, in begin
    version, status, reason = self._read_status()
  File &quot;/usr/lib/python2.7/httplib.py&quot;, line 365, in _read_status
    line = self.fp.readline()
  File &quot;/usr/lib/python2.7/socket.py&quot;, line 447, in readline
    data = self._sock.recv(self._rbufsize)
timeout: timed out

Connecting to	 http://www.tdbrecords.com
Connecting to	 http://www.electrichumanproject.com/
Connecting to	 http://tweekerchick.blogspot.com/
Connecting to	 http://www.besound.com/pushead/home.html
Connecting to	 http://www.porkchopscreenprinting.com/
Connecting to	 http://www.kinseyvisual.com
Connecting to	 http://www.rathergood.com
Connecting to	 http://www.lepoint.fr/
Connecting to	 http://www.revhq.com
Connecting to	 http://www.poprocksandcoke.com
Connecting to	 http://www.samuraiblue.com/
Connecting to	 http://www.openbsd.org/cgi-bin/man.cgi
Connecting to	 http://www.sysblog.com
Connecting to	 http://www.voicesofsafety.com
</code></pre>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_connection = pd.concat(frames)
</code></pre></div><div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_connection.set_index([&#39;url&#39;, &#39;response&#39;, &#39;header&#39;])
</code></pre></div><!-- raw HTML omitted -->
<h2 id="save-to-excel">Save to Excel</h2>
<p>Now feel free to do whatever you want with your DataFrames: Export them to CSV, EXCEL, TXT etc.</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">from pandas import ExcelWriter
writer = ExcelWriter(&#39;Excel/output.xls&#39;)
df_whois.to_excel(writer, &#34;Sheet - WHOIS&#34;)
df_dns.to_excel(writer, &#34;Sheet - DNS&#34;)
#df_connection.to_excel(writer, &#34;Sheet - Connections&#34;)
</code></pre></div><p>Since I wasn&rsquo;t able to export the <code>df_connection</code> to Excel (<code>Exception: Unexpected data type &lt;class 'socket.timeout'&gt;</code>) I had to export it to CSV:</p>
<div class="highlight"><pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">df_connection.to_csv(&#34;Excel/connection.csv&#34;, sep=&#34;\t&#34;, header=True)
</code></pre></div></div>
</main>
 






<div class="tl fixed list-pages lh-copy" id="contents-list"></div>



<div class="pagination tc tr-l db fixed-l bottom-2-l right-2-l mb3 mb0-l">
  
<a id="scroll-to-top" class="f6 o-0 link br2 ph2 pv1 mb1 bg-main-color pointer" onclick="topFunction()" style="color: #fff; visibility: hidden; display: none; transition: opacity .5s, visibility .5s;" title="back to top">back to top</a>
<br>
  <p class="mb0 mt2">
  <a href="http://blog.dornea.nu/2014/08/05/android-dynamic-code-analysis-mastering-droidbox/">prev post</a>
  <a href="http://blog.dornea.nu/2014/08/06/python-for-data-analysis/">next post</a>
  </p>
</div>

  <footer class="content-width mt0 mt5-l mb4 f6 center ph3 gray tc tl-l">
  <hr class="dn db-l ml0-l gray w3"><br>
  Powered by <a href="https://gohugo.io/" target="_blank" class="link gray dim">Hugo</a>, based on the <a href="https://github.com/lingxz/er" target="_blank" class="link gray dim">Er</a> theme. <br>
  
</footer>
  



<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
<style>.is-active-link::before { background-color: var(--secondary-color); }</style>




<script type="text/javascript">
var prevScrollpos = window.pageYOffset;
window.onscroll = function() {
  var currentScrollPos = window.pageYOffset;

  
  if (document.getElementById("tag-cloud") !== null) { 
    if (prevScrollpos > currentScrollPos) { 
      document.getElementById("tag-cloud").style.visibility = "visible";
      document.getElementById("tag-cloud").style.opacity = "1";
    } else {
      document.getElementById("tag-cloud").style.visibility = "hidden";
      document.getElementById("tag-cloud").style.opacity = "0";
    }
  }
  

  
  if (document.body.scrollTop > 1000 || document.documentElement.scrollTop > 1000) {
      document.getElementById("scroll-to-top").style.display = "inline";
      document.getElementById("scroll-to-top").style.visibility = "visible";
      document.getElementById("scroll-to-top").style.opacity = "1";
  } else {
      document.getElementById("scroll-to-top").style.visibility = "hidden";
      document.getElementById("scroll-to-top").style.opacity = "0";
  }
  
  prevScrollpos = currentScrollPos;
}


function topFunction() {
  document.body.scrollTop = 0; 
  document.documentElement.scrollTop = 0; 
}






if (document.getElementById("contents-list") !== null && document.getElementsByClassName("post-content").length !== 0) { 
  tocbot.init({
    
    tocSelector: '#contents-list',
    
    contentSelector: '.post-content',
    
    headingSelector: 'h1, h2, h3',
  });
}


</script>




</body>
</html>